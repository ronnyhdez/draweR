---
title: "Data Table use cases"
author: "Ronny A. Hern√°ndez Mora"
date: '2022-06-21'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Translating a dplyr pipeline to data table

The problem was that a project had local csv files with many rows. When
cleaning the data using dplyr this would consume a lot of the memory and take
a lot of time to produce a result.

So, an option was to implement the package `data.table` to be able to handle
all this data in a local computer.


This was the original data pipeline:

```{r, eval = FALSE}
pixels_clean <- pixels %>% 
  separate(file_id, into = c("folder", "file_id"), sep = "/") %>% 
  mutate(file_id = str_remove(file_id, ".csv")) %>% 
  mutate(location = str_extract(geo, "\\[(.*?)\\]") ) %>% 
  separate(col = "location", into = c("lat", "long"), sep = ",") %>% 
  mutate(lat = str_extract(lat, "-?[0-9.]+"),
         long = str_extract(long, "-?[0-9.]+")) %>% 
  select(-folder, -geo) %>% 
  separate(file_id, into = c("first", "second", "tnk"), remove = FALSE) %>% 
  select(-second, -tnk) %>% 
  separate(first, into = c("date", "time"), sep = "T") %>% 
  mutate(date = ymd(date))
```

The translation to data table:

```{r, eval = FALSE}
setDT(pixels_sr)
pixels_sr[, c("folder_1", "folder_2", "file_id") := tstrsplit(file_id, "/", fixed = TRUE)]
pixels_sr[ , ":="(file_id = str_remove(file_id, ".csv"))]
pixels_sr[ , ":="(location = str_extract(geo, "\\[(.*?)\\]"))]
pixels_sr[,  c("lat", "long") := tstrsplit(location, ",", fixed = TRUE)]
pixels_sr[ , ":="(lat = str_extract(lat, "-?[0-9.]+"),
                  long = str_extract(long, "-?[0-9.]+"))]
pixels_sr[, ":="(date = ymd(file_id))]
pixels_sr[, c("file_id", "system_index", 
              "folder_1", "folder_2",
              "geo", "location") := NULL]
```


